{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from https://github.com/andreagrandi/ml-pima-notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    " The Pima are a group of Native Americans living in Arizona. A genetic predisposition allowed this group to survive normally to a diet poor of carbohydrates for years. In the recent years, because of a sudden shift from traditional agricultural crops to processed foods, together with a decline in physical activity, made them develop the highest prevalence of type 2 diabetes and for this reason they have been subject of many studies.\n",
    "\n",
    "## Dataset\n",
    " The dataset includes data from 768 women with 8 characteristics, in particular:\n",
    "\n",
    "* Number of times pregnant\n",
    "* Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "* Diastolic blood pressure (mm Hg)\n",
    "* Triceps skin fold thickness (mm)\n",
    "* 2-Hour serum insulin (mu U/ml)\n",
    "* Body mass index (weight in kg/(height in m)^2)\n",
    "* Diabetes pedigree function\n",
    "* Age (years)\n",
    "*The last column of the dataset indicates if the person has been diagnosed with diabetes (1) or not (0)\n",
    "\n",
    "## Source\n",
    " The original dataset is available at UCI Machine Learning Repository and can be downloaded from this address: http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes\n",
    "\n",
    "## The problem\n",
    " The type of dataset and problem is a classic supervised binary classification. Given a number of elements all with certain characteristics (features), we want to build a machine learning model to identify people affected by type 2 diabetes.\n",
    "\n",
    " To solve the problem we will have to analyse the data, do any required transformation and normalisation, apply a machine learning algorithm, train a model, check the performance of the trained model and iterate with other algorithms until we find the most performant for our type of dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the libraries needed to read the dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We placed the dataset under datasets/ sub folder\n",
    "DATASET_PATH = '~/ai/datasets/pima-indians-diabetes/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We read the data from the CSV file\n",
    "data_path = os.path.join(DATASET_PATH, 'pima-indians-diabetes.csv')\n",
    "dataset = pd.read_csv(data_path, header=None)\n",
    "\n",
    "# Because thr CSV doesn't contain any header, we add column names \n",
    "# using the description from the original dataset website\n",
    "dataset.columns = [\n",
    "    \"Pregnancies\", \"Glucose\", \"Blood_Pressure\",\n",
    "    \"Skin_Thickness\", \"Insulin\", \"BMI\",\n",
    "    \"Diabetes_Pedigree_Function\", \"Age\", \"Has_Diabetes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the data: we have 768 rows and 9 columns:\n",
    "# the first 8 columns are features while the last one\n",
    "# is the supervised label (1 = has diabetes, 0 = no diabetes)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise a table with the first rows of the dataset, to\n",
    "# better understand the data format\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data correlation matrix\n",
    " The correlation matrix is an important tool to understand the correlation between the different characteristics. The values range from -1 to 1 and the closer a value is to 1 the better correlation there is between two characteristics. Let's calculate the correlation matrix for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Data Correlation Matrix\n",
    "corr = dataset.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not a doctor and I don't have any knowledge of medicine, but from the data I can guess that the greater the age or the BMI of a patient is, the greater probabilities are the patient can develop type 2 diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a heatmap to represent the correlation\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.heatmap(corr, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise the Dataset\n",
    " Visualising the data is an important step of the data analysis. With a graphical visualisation of the data we have a better understanding of the various features values distribution: for example we can understand what's the average age of the people or the average BMI etc...\n",
    "\n",
    " We could of course limit our inspection to the table visualisation, but we could miss important things that may affect our model precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the dataset\n",
    "import matplotlib.pyplot as plt\n",
    "dataset.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing I notice in the dataset (and that wasn't obvious at the beginning) is the fact that some people have null (zero) values for some of the features: it's not quite possible to have 0 as BMI or for the blood pressure.\n",
    "\n",
    "How can we deal with similar values? We will see it later during the data transformation phase.\n",
    "\n",
    "## Data cleaning and transformation\n",
    "We have noticed from the previous analysis that some patients have missing data for some of the features. Machine learning algorithms don't work very well when the data is missing so we have to find a solution to \"clean\" the data we have.\n",
    "\n",
    "The easiest option could be to eliminate all those patients with null/zero values, but in this way we would eliminate a lot of important data.\n",
    "\n",
    "Another option is to calculate the median value for a specific column and substitute that value everywhere (in the same column) we have zero or null. Let's see how to apply this second method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median value for BMI\n",
    "median_bmi = dataset['BMI'].median()\n",
    "# Substitute it in the BMI column of the\n",
    "# dataset where values are 0\n",
    "dataset['BMI'] = dataset['BMI'].replace(\n",
    "    to_replace=0, value=median_bmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median value for Blood_Pressure\n",
    "median_blood_pressure = dataset['Blood_Pressure'].median()\n",
    "# Substitute it in the Blood_Pressure column of the\n",
    "# dataset where values are 0\n",
    "dataset['Blood_Pressure'] = dataset['Blood_Pressure'].replace(\n",
    "    to_replace=0, value=median_blood_pressure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median value for Glucose\n",
    "median_glucose = dataset['Glucose'].median()\n",
    "# Substitute it in the Glucose column of the\n",
    "# dataset where values are 0\n",
    "dataset['Glucose'] = dataset['Glucose'].replace(\n",
    "    to_replace=0, value=median_glucose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median value for Skin_Thickness\n",
    "median_skin_thickness = dataset['Skin_Thickness'].median()\n",
    "# Substitute it in the Skin_Thickness column of the\n",
    "# dataset where values are 0\n",
    "dataset['Skin_Thickness'] = dataset['Skin_Thickness'].replace(\n",
    "    to_replace=0, value=median_skin_thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median value for Insulin\n",
    "median_insulin = dataset['Insulin'].median()\n",
    "# Substitute it in the Insulin column of the\n",
    "# dataset where values are 0\n",
    "dataset['Insulin'] = dataset['Insulin'].replace(\n",
    "    to_replace=0, value=median_insulin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I haven't transformed all the columns, because for some values can make sense to be zero (like \"Number of times pregnant\").\n",
    "\n",
    "## Splitting the Dataset\n",
    " Now that we have transformed the data we need to split the dataset in two parts: a training dataset and a test dataset. Splitting the dataset is a very important step for supervised machine learning models. Basically we are going to use the first part to train the model (ignoring the column with the pre assigned label), then we use the trained model to make predictions on new data (which is the test dataset, not part of the training set) and compare the predicted value with the pre assigned label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset in 80% / 20%\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(\n",
    "    dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate labels from the rest of the dataset\n",
    "train_set_labels = train_set[\"Has_Diabetes\"].copy()\n",
    "train_set = train_set.drop(\"Has_Diabetes\", axis=1)\n",
    "\n",
    "test_set_labels = test_set[\"Has_Diabetes\"].copy()\n",
    "test_set = test_set.drop(\"Has_Diabetes\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "One of the most important data transformations we need to apply is the features scaling. Basically most of the machine learning algorithms don't work very well if the features have a different set of values. In our case for example the Age ranges from 20 to 80 years old, while the number of times a patient has been pregnant ranges from 0 to 17. For this reason we need to apply a proper transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a scaler\n",
    "from sklearn.preprocessing import MinMaxScaler as Scaler\n",
    "\n",
    "scaler = Scaler()\n",
    "scaler.fit(train_set)\n",
    "train_set_scaled = scaler.transform(train_set)\n",
    "test_set_scaled = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output scaled values\n",
    "df = pd.DataFrame(data=train_set_scaled)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and train a model\n",
    "It's not possible to know in advance which algorithm will work better with our dataset. We need to compare a few and select the one with the \"best score\".\n",
    "\n",
    "## Comparing multiple algorithms\n",
    "To compare multiple algorithms with the same dataset, there is a very nice utility in sklearn called model_selection. We create a list of algorithms and then we score them using the same comparison method. At the end we pick the one with the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the algorithms we want to test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sklearn utility to compare algorithms\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare an array with all the algorithms\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVC', SVC()))\n",
    "models.append(('LSVC', LinearSVC()))\n",
    "models.append(('RFC', RandomForestClassifier()))\n",
    "models.append(('DTR', DecisionTreeRegressor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the configuration to run the test\n",
    "seed = 7\n",
    "results = []\n",
    "names = []\n",
    "X = train_set_scaled\n",
    "Y = train_set_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every algorithm is tested and results are\n",
    "# collected and printed\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(\n",
    "        n_splits=10, random_state=seed, shuffle=True)\n",
    "    cv_results = model_selection.cross_val_score(\n",
    "        model, X, Y, cv=kfold, scoring='accuracy')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (\n",
    "        name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like that using this comparison method, the most performant algorithm is SVC.\n",
    "\n",
    "## Find the best parameters for SVC\n",
    "The default parameters for an algorithm are rarely the best ones for our dataset. Using sklearn we can easily build a parameters grid and try all the possible combinations. At the end we inspect the best_estimator_ property and get the best ones for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1.0, 10.0, 50.0],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'shrinking': [True, False],\n",
    "    'gamma': ['auto', 1, 0.1],\n",
    "    'coef0': [0.0, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "model_svc = SVC()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    model_svc, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(train_set_scaled, train_set_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the bext score found\n",
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the parameters to the model and train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the algorithm using parameters\n",
    "# from best_estimator_ property\n",
    "svc = grid_search.best_estimator_\n",
    "\n",
    "# Use the whole dataset to train the model\n",
    "X = np.append(train_set_scaled, test_set_scaled, axis=0)\n",
    "Y = np.append(train_set_labels, test_set_labels, axis=0)\n",
    "\n",
    "# Train the model\n",
    "svc.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a new (fake) person having the three most correlated values high (Glucose, BMI and Age)\n",
    "new_df = pd.DataFrame([[6, 168, 72, 35, 0, 43.6, 0.627, 65]])\n",
    "# We scale those values like the others\n",
    "new_df_scaled = scaler.transform(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predict the outcome\n",
    "prediction = svc.predict(new_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A value of \"1\" means that this person is likley to have type 2 diabetes\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We finally find a score of 76% using SVC algorithm and parameters optimisation. Please note that there may be still space for further analysis and optimisation, for example trying different data transformations or trying algorithms that haven't been tested yet. Once again I want to repeat that training a machine learning model to solve a problem with a specific dataset is a try / fail / improve process.\n",
    "\n",
    "## Credits\n",
    "First of all I need to thank my wife Dr Daniela Ceccarelli Ceccarelli for helping me to validate this experiment and for checking I didn't write anything wrong from a medical point of view. Then I want to thank Dr. Jason Brownlee for his fantastic blog which has helped me a lot to understand many concepts used here. I strongly advise you to have a look at his blog: https://machinelearningmastery.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
